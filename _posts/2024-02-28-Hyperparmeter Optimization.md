---
layout: post
title: Hyperparameter Optimization
date: 2025-02-28
categories: [Machine Learning, Theory]
tags: [Hyperparameter, Grid Search, Random Search, Bayesian Optimization, Optuna]
---

# ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ²• μ¤‘ λ€ν‘μ μΈ 4κ°€μ§€ λ°©λ²•
ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ²• μ¤‘ λ€ν‘μ μΈ 4κ°€μ§€ λ°©λ²•μ—λ” Grid Search, Random Search, Bayesian Optimization, Optunaκ°€ μλ‹¤. 

ν•μ΄νΌνλΌλ―Έν„°(Hyperparameter)λ€ λ¨λΈ ν•™μµ μ „ μ‚¬μ©μκ°€ μ§μ ‘ μ„¤μ •ν•΄μ•Ό ν•λ” κ°’μΌλ΅, λ¨λΈμ μ„±λ¥κ³Ό ν•™μµ κ³Όμ •μ— ν° μν–¥μ„ λ―ΈμΉλ‹¤. μ΄λ” λ¨λΈμ΄ ν•™μµμ„ ν†µν•΄ μλ™μΌλ΅ μµμ ν™”ν•λ” νλΌλ―Έν„°μ™€ κµ¬λ³„λλ‹¤.
### ν•μ΄νΌνλΌλ―Έν„° VS νλΌλ―Έν„° λΉ„κµ
| κµ¬λ¶„ | ν•μ΄νΌνλΌλ―Έν„° (Hyperparameter) | νλΌλ―Έν„° (Parameter) |
|------|--------------------------------|----------------------|
| **μ •μ** | μ‚¬μ©μκ°€ μ§μ ‘ μ„¤μ •ν•λ” κ°’ | λ¨λΈμ΄ ν•™μµμ„ ν†µν•΄ μλ™μΌλ΅ μµμ ν™”ν•λ” κ°’ |
| **μμ‹** | ν•™μµλ¥  (Learning Rate), λ°°μΉ ν¬κΈ° (Batch Size), μ€λ‹‰μΈµ κ°μ | κ°€μ¤‘μΉ (Weight), νΈν–¥ (Bias) |
| **μ΅°μ • λ°©λ²•** | Grid Search, Random Search, Bayesian Optimization | μ—­μ „ν (Backpropagation), Gradient Descent |
| **ν•™μµ κ³Όμ •** | ν•™μµ μ „μ— κ²°μ •λ¨ | ν•™μµν•λ©΄μ„ μ—…λ°μ΄νΈλ¨ |

## ν•μ΄ν—νλΌλ―Έν„° μµμ ν™”μ ν•„μ”μ„±

ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μµμ ν™”ν•λ©΄ λ¨λΈμ μ„±λ¥μ„ κ·Ήλ€ν™”ν•κ³  μΌλ°ν™” μ„±λ¥μ„ ν–¥μƒμ‹ν‚¬ μ μλ‹¤. μ μ ν• ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ„¤μ •ν•μ§€ μ•μΌλ©΄ κ³Όμ ν•©(Overfitting) λλ” κ³Όμ†μ ν•©(Underfitting)μ΄ λ°μƒν•  μ μμΌλ©°, λ¨λΈμ μ„±λ¥μ΄ μ €ν•λλ‹¤.


- λ¨λΈ μ„±λ¥ κ·Ήλ€ν™”
  - μ μ΅°μ •λ ν•μ΄νΌνλΌλ―Έν„°λ” λ¨λΈμ μμΈ΅ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚¬ μ μμ
  - ν•™μµλ¥ (learning rate),  λ°°μΉν¬κΈ°(batch size), μ •κ·ν™” κ³„μ(lambda) λ“±μ€ λ¨λΈμ μ„±λ¥μ— μ§μ ‘μ μΈ μν–¥μ„ λ―ΈμΉ¨
- κ³Όμ ν•©(Overfitting) λ°©μ§€
- κ³Όμ†μ ν•©(Underfitting) λ°©μ§€
- ν•™μµ μ†λ„ μµμ ν™”
- μΌλ°ν™” μ„±λ¥ ν–¥μƒ

## μ£Όμ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•
![ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λΉ„κµ](/assets/images/hyperparameter_optimization.png)

1. Grid Search
- λ―Έλ¦¬ μ •μλ ν•μ΄νΌνλΌλ―Έν„° ν›„λ³΄ κ°’λ“¤μ λ¨λ“  μ΅°ν•©μ„ νƒμƒ‰ν•λ” λ°©μ‹
- Ex) λ‘ κ°μ ν•μ΄νΌνλΌλ―Έν„°  (x, y) μ— λ€ν•΄  x \in [0.1, 0.01, 0.001] ,  y \in [1, 10, 100] μ΄λ©΄ μ΄  3 \times 3 = 9 κ°μ κ²½μ°λ¥Ό νƒμƒ‰

μ¥μ 
- νƒμƒ‰μ΄ μ²΄κ³„μ μΌλ΅ μ΄λ£¨μ–΄μ Έ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ³΄μ¥ν•  μ μμ
- μ„¤μ •ν• λ²”μ„ λ‚΄μ—μ„ ::*λ¨λ“  μ΅°ν•©μ„ νƒμƒ‰ν•λ―€λ΅ μ¬ν„μ„± λ†’μ*::

λ‹¨μ 
- μ΅°ν•©μ΄ λ§μ•„μ§μλ΅ νƒμƒ‰ μ‹κ°„μ΄ κΈ‰κ²©ν μ¦κ°€(μ°¨μ›μ μ €μ£Ό)
- μ¤‘μ”ν• ν•μ΄νΌνλΌλ―Έν„° μμ—­λ³΄λ‹¤ μ  μ¤‘μ”ν• μμ—­μ—λ„ κ°™μ€ μ—°μ‚° μν–‰

2. Random Search
- ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„ λ¬΄μ‘μ„λ΅ κ°’μ„ μ„ νƒν•μ—¬ νƒμƒ‰ν•λ” λ°©μ‹
- μΌμ •ν• λ°λ³µ νμ(N) λ™μ• λλ¤ν•κ² ν•μ΄νΌνλΌλ―Έν„° κ°’μ„ μ„ νƒν•μ—¬ λ¨λΈμ„ ν‰κ°€

μ¥μ 
- νƒμƒ‰ κ³µκ°„μ΄ μ»¤μ§μλ΅ Grid Searchλ³΄λ‹¤ ν¨μ¨μ 
- νΉμ • μμ—­μ—μ„ μΆ‹μ€ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύμ„ ν™•λ¥ μ΄ λ†’μ
- λ³‘λ ¬ μ²λ¦¬κ°€ μ©μ΄ν•μ—¬ μ‹¤ν–‰ μ†λ„λ¥Ό λΉ λ¥΄κ² ν•  μ μμ

λ‹¨μ 
- μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ³΄μ¥ν•μ§€ μ•μ
- λ¶ν•„μ”ν• νƒμƒ‰μ΄ λ°μƒν•  μ μμ

Grid Searchλ” κ³ μ •λ κ·Έλ¦¬λ“μ—μ„ μƒν”λ§ν•μ§€λ§, Random Searchλ” λλ¤ν•κ² μƒν”λ§ν•μ—¬ λ” λ„“μ€ μμ—­μ„ ν¨μ¨μ μΌλ΅ νƒμƒ‰ν•  μ μμ

Ex) 100κ°μ μ‹¤ν—μ„ ν•λ‹¤λ©΄, Grid Searchλ” νΉμ •ν• 100κ°μ μ„μΉμ—μ„ ν‰κ°€ν•μ§€λ§, Random Searchλ” λ” κ³ λ¥΄κ² λ¶„ν¬λ 100κ° μ„μΉμ—μ„ ν‰κ°€λ¨.

3. Bayesian Optimization(λ² μ΄μ§€μ• μµμ ν™”)
- μ΄μ „ μ‹¤ν— κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ ν™•λ¥  λ¨λΈ(Gaussian Process, Tree-based λ¨λΈ λ“±)μ„ ν•™μµν•μ—¬ νƒμƒ‰μ„ ν¨μ¨μ μΌλ΅ μν–‰ν•λ” λ°©μ‹
- νƒμƒ‰λ„ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό ν™•λ¥ μ μΌλ΅ μμΈ΅ν•μ—¬ μ λ§ν• μμ—­μ„ μ§‘μ¤‘μ μΌλ΅ νƒμƒ‰

μ¥μ 
- κΈ°μ΅΄ μ •λ³΄(μ΄μ „ κ²°κ³Ό)λ¥Ό ν™μ©ν•μ—¬ ν¨μ¨μ μΈ νƒμƒ‰ κ°€λ¥
- μ—°μ‚° λΉ„μ©μ„ μ¤„μ΄λ©΄μ„λ„ μΆ‹μ€ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύμ„ κ°€λ¥μ„±μ΄ λ†’μ
- Random Searchλ³΄λ‹¤ μ μ€ μ—°μ‚°μΌλ΅ λ” λ‚μ€ κ²°κ³Όλ¥Ό μ°Ύμ„ κ°€λ¥μ„±μ΄ νΌ

λ‹¨μ 
- κµ¬ν„μ΄ λ³µμ΅ν•κ³  κ³„μ‚°λ‰ λ§μ„ μ μμ
- κ³ μ°¨μ› κ³µκ°„μ—μ„ νƒμƒ‰μ΄ μ–΄λ ¤μΈ μ μμ

4. Optuna
- Bayesian Optimizationμ„ κΈ°λ°μΌλ΅ ν•λ” Python ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λΌμ΄λΈλ¬λ¦¬
- Tree-structured Parzen Estimator(TPE) μ•κ³ λ¦¬μ¦μ„ μ‚¬μ©ν•μ—¬ Bayesian Optimizationλ³΄λ‹¤ ν¨μ¨μ μΈ νƒμƒ‰μ„ μ§€μ›
- Grid Search, Random Search, Bayesian Optimizationμ λ‹¨μ μ„ λ³΄μ™„ν•μ—¬ λ‹¤μ–‘ν• μ „λµμ„ μ§€μ›

μ¥μ 
- ν•μ΄νΌνλΌλ―Έν„° μ¤‘μ”λ„λ¥Ό μλ™μΌλ΅ ν•™μµν•μ—¬ μµμ ν™”
- Early Stoppingκ³Ό κ°™μ€ κΈ°λ¥μ„ ν™μ©ν•μ—¬ λ¶ν•„μ”ν• κ³„μ‚° μ¤„μ„
- λ³‘λ ¬ μ²λ¦¬λ¥Ό μ§€μ›ν•μ—¬ κ³„μ‚° μ†λ„ λΉ λ¥΄κ² ν•  μ μμ
- Pruning(λΉ„ν¨μ¨μ μΈ νƒμƒ‰ μ΅°κΈ° μΆ…λ£) κΈ°λ¥ μ§€μ›

λ‹¨μ 
- ν•™μµ κ³΅μ„ μ΄ μΌμ •ν•μ§€ μ•μ„ μ μμ
- Bayesian Optimization λ³΄λ‹¤ λ” λ§μ€ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ΄ ν•„μ”ν•  μ μμ

**μ‹¤λ¬΄μ—μ„λ” Grid Searchλ³΄λ‹¤λ” Random Search λλ” Bayesian Optimizationμ„ λ” λ§μ΄ μ‚¬μ©ν•λ©°, μµκ·Όμ—λ” Optuna κ°™μ€ λΌμ΄λΈλ¬λ¦¬λ¥Ό ν™μ©ν•μ—¬ μλ™ν™”λ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό μν–‰ν•λ” κ²ƒμ΄ μΌλ°μ μ΄λ‹¤.**


### 1. Grid Search
κ·Έλ¦¬λ“μ„μΉ(Grid Search)λ” λ¨Έμ‹ λ¬λ‹ λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ—μ„ μ‚¬μ©λλ” κΈ°λ²•μΌλ΅, λ¨λ“  κ°€λ¥ν• ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ²΄κ³„μ μΌλ΅ νƒμƒ‰ν•μ—¬ μµμ μ μ΅°ν•©μ„ μ°Ύλ” λ°©λ²•μ΄λ‹¤. μ΄ λ°©λ²•μ€ κ° ν•μ΄νΌνλΌλ―Έν„°μ— λ€ν•΄ λ―Έλ¦¬ μ •μλ κ°’λ“¤μ μ§‘ν•©μ„ μƒμ„±ν•κ³ , μ΄λ“¤μ λ¨λ“  κ°€λ¥ν• μ΅°ν•©μ„ ν‰κ°€ν•μ—¬ λ¨λΈ μ„±λ¥μ„ κ·Ήλ€ν™”ν•λ” λ° μ‚¬μ©λλ‹¤.

Grid Searchλ” μ „μ μ΅°μ‚¬(exhaustive search) λ°©μ‹μ„ μ±„νƒν•μ—¬, κ°€λ¥ν• λ¨λ“  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ ν‰κ°€ν•λ‹¤. μ΄λ¬ν• μ ‘κ·Ό λ°©μ‹μ€ νƒμƒ‰ κ³µκ°„(search space) λ‚΄μ λ¨λ“  μ§€μ μ„ κ²©μ ν•νƒλ΅ λ‚λ„μ–΄ κ° μ§€μ μ—μ„ λ¨λΈμ„ ν•™μµμ‹ν‚¤κ³  κ²€μ¦ν•μ—¬ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύλ”λ‹¤.

Grid Searchμ νΉμ§•
- μ²΄κ³„μ  νƒμƒ‰: λ¨λ“  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μΌμ •ν• κ°„κ²©μΌλ΅ νƒμƒ‰ν•μ—¬ μµμ μ μ΅°ν•©μ„ μ°Ύλ”λ‹¤
- λ³‘λ ¬ν™” κ°€λ¥: κ° μ΅°ν•©μ ν‰κ°€κ°€ λ…λ¦½μ μ΄λ―€λ΅ λ³‘λ ¬ μ²λ¦¬λ¥Ό ν†µν•΄ νƒμƒ‰ μ‹κ°„μ„ λ‹¨μ¶•ν•  μ μλ‹¤
- κ³„μ‚° λΉ„μ©: ν•μ΄νΌνλΌλ―Έν„°μ μμ™€ κ° ν•μ΄νΌνλΌλ―Έν„°μ ν›„λ³΄ κ°’μ΄ λ§μ„μλ΅ κ³„μ‚° λΉ„μ©μ΄ κΈ°ν•κΈ‰μμ μΌλ΅ μ¦κ°€ν•λ” λ‹¨μ μ΄ μλ‹¤

Grid Searchλ” λ‹¨μν•κ³  κµ¬ν„μ΄ μ©μ΄ν•μ§€λ§, κ³ μ°¨μ›μ ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„λ” κ³„μ‚° λΉ„μ©μ΄ κΈ‰κ²©ν μ¦κ°€ν•λ” λ¬Έμ κ°€ μλ‹¤. μ΄λ¬ν• λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ Random Searchλ‚ Bayesian Optimizationκ³Ό κ°™μ€ λ°©λ²•μ΄ μ μ•λμ—λ‹¤. νΉν Random Searchλ” ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„ λ¬΄μ‘μ„λ΅ μ΅°ν•©μ„ μ„ νƒν•΄ νƒμƒ‰ν•λ” λ°©λ²•μΌλ΅, κ³ μ°¨μ› κ³µκ°„μ—μ„ λ” ν¨μ¨μ μΌ μ μλ‹¤. λν• Bayesian Optimizationμ€ μ΄μ „μ νƒμƒ‰ κ²°κ³Όλ¥Ό κΈ°λ°μΌλ΅ ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ„ λ¨λΈλ§ν•μ—¬ ν¨μ¨μ μΌλ΅ μµμ μ μ΅°ν•©μ„ μ°Ύλ” λ°©λ²•μ΄λ‹¤.

κ²°κµ­ Grid Searchλ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ κΈ°λ³Έμ μΈ λ°©λ²•μΌλ΅, λ¨λ“  κ°€λ¥ν• μ΅°ν•©μ„ μ²΄κ³„μ μΌλ΅ νƒμƒ‰ν•μ—¬ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύλ”λ‹¤. κ·Έλ¬λ‚ νƒμƒ‰ κ³µκ°„μ΄ μ»¤μ§μλ΅ κ³„μ‚° λΉ„μ©μ΄ μ¦κ°€ν•λ―€λ΅, μ΄λ° κ²½μ° Random Searchλ‚ Bayesian Optimizationκ³Ό κ°™μ€ λ€μ•μ μΈ λ°©λ²•μ„ κ³ λ ¤ν•λ” κ²ƒμ΄ μΆ‹λ‹¤.


### 2. Random Search
ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„ λ¬΄μ‘μ„λ΅ μ΅°ν•©μ„ μ„ νƒν•μ—¬ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύλ” λ°©λ²•μΌλ΅, Grid Searchμ— λΉ„ν•΄ κ³„μ‚° ν¨μ¨μ„±μ„ λ†’μΌ μ μλ” μ¥μ μ΄λ‹¤.

μ „ν†µμ μΌλ΅ Grid Searchκ°€ μ‚¬μ©λμ–΄ μ™”μΌλ‚, ν•μ΄νΌνλΌλ―Έν„°μ μμ™€ κ° ν•μ΄νΌνλΌλ―Έν„°μ ν›„λ³΄ κ°’μ΄ λ§μ„μλ΅ κ³„μ‚° λΉ„μ©μ΄ κΈ°ν•κΈ‰μμ μΌλ΅ μ¦κ°€ν•λ” λ¬Έμ κ°€ μλ‹¤. μ΄λ¬ν• λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ Bergstraμ™€ Bengio(2012)λ” Random Searchλ¥Ό μ μ•ν–λ‹¤. μ΄λ“¤μ€ Random Search for Hyper-Parameter Optimization λ…Όλ¬Έμ—μ„, Random Searchκ°€ Grid Searchλ³΄λ‹¤ ν¨μ¨μ μ΄κ³  ν¨κ³Όμ μΌ μ μμμ„ μ…μ¦ν–λ‹¤.

Random Search νΉμ§•
- ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ μΌλ¶€λ¥Ό λ¬΄μ‘μ„λ΅ μƒν”λ§ν•μ—¬ νƒμƒ‰ν•λ―€λ΅, κ³„μ‚° λΉ„μ© μ¤„μΌ μ μμ
- λ¬΄μ‘μ„ μƒν”λ§μ„ ν†µν•΄ ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ λ‹¤μ–‘ν• μμ—­ νƒμƒ‰ κ°€λ¥
- κµ¬ν„μ΄ κ°„λ‹¨ν•λ©°, λ³‘λ ¬ν™”κ°€ μ©μ΄ν•¨

Random Searchλ” ν¨μ¨μ μ΄μ§€λ§, νƒμƒ‰ κ³Όμ •μ—μ„ μ¤‘μ”ν• ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ λ†“μΉ  μ μλ” κ°€λ¥μ„±μ΄ μλ‹¤. μ΄λ¬ν• ν•κ³„λ¥Ό λ³΄μ™„ν•κΈ° μ„ν•΄ Bayesian Optimizationκ³Ό κ°™μ€ κΈ°λ²•μ΄ μ μ•λμ—λ‹¤. Bayesian Optimizationμ€ μ΄μ „μ νƒμƒ‰ κ²°κ³Όλ¥Ό κΈ°λ°μΌλ΅ ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ„ λ¨λΈλ§ν•μ—¬, λ” ν¨μ¨μ μΌλ΅ μµμ μ μ΅°ν•©μ„ μ°Ύλ” λ°©λ²•μ΄λ‹¤.

κ²°λ΅ μ€, Random Searchλ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ—μ„ λ‹¨μν•λ©΄μ„λ„ ν¨μ¨μ μΈ λ°©λ²•μΌλ΅, νΉν ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ΄ ν΄ λ• μ μ©ν•λ‹¤. κ·Έλ¬λ‚ μ¤‘μ”ν• μ΅°ν•©μ„ λ†“μΉ  μ μλ” κ°€λ¥μ„±μ΄ μμ–΄ Bayesian Optimizationκ³Ό κ°™μ€ κΈ°λ²•κ³Ό ν•¨κ» μ‚¬μ©ν•μ—¬ μµμ ν™”λ¥Ό μν–‰ν•λ” κ²ƒμ΄ λ°”λμ§ν•λ‹¤.

### 3. Bayesian Optimization
λΉ„μ©μ΄ λ§μ΄ λ“λ” λΈ”λ™λ°•μ¤ ν•¨μμ μ „μ—­ μµμ ν™”λ¥Ό μ„ν• ν¨μ¨μ μΈ λ°©λ²•μΌλ΅, ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ“± λ‹¤μ–‘ν• λ¶„μ•Όμ—μ„ ν™μ©λκ³  μλ‹¤. μ΄ κΈ°λ²•μ€ λ² μ΄μ§€μ• ν†µκ³„ν•™μ„ κΈ°λ°μΌλ΅, μ„λ΅κ²μ΄νΈ λ¨λΈ(surrogate model)κ³Ό νλ“ ν•¨μ(acquisition function)λ¥Ό μ‚¬μ©ν•μ—¬ λ©μ  ν•¨μμ μµμ†κ°’ λλ” μµλ€ κ°’μ„ μ°Ύλ”λ‹¤.

λ² μ΄μ§€μ• μµμ ν™”λ” λ―Έμ§€μ λ©μ  ν•¨μλ¥Ό ν™•λ¥ μ  λ¨λΈλ΅ μ·¨κΈ‰ν•μ—¬, μ£Όμ–΄μ§„ λ°μ΄ν„°μ— λ”°λΌ μ—…λ°μ΄νΈλλ” μ‚¬ν›„ λ¶„ν¬(posterior distribution)λ¥Ό ν•μ„±ν•λ‹¤. μ΄λ¬ν• μ ‘κ·Όμ€ λ² μ΄μ§€μ• μ •λ¦¬λ¥Ό κΈ°λ°μΌλ΅ ν•λ©°, μƒλ΅μ΄ λ°μ΄ν„°κ°€ μ¶”κ°€λ  λ•λ§λ‹¤ λ¨λΈμ΄ μ—…λ°μ΄νΈλμ–΄ λ©μ  ν•¨μμ ν•νƒλ¥Ό μ μ§„μ μΌλ΅ μ¶”μ •ν•λ‹¤. μ΄λ¥Ό ν†µν•΄ ν•¨μ ν‰κ°€ νμλ¥Ό μµμ†ν™”ν•λ©΄μ„ μµμ μ μ„ μ°Ύμ•„λ‚Ό μ μλ‹¤. 

Bayesian Optimizationμ κµ¬μ„± μ”μ†
- 1. μ„λ΅κ²μ΄νΈ λ¨λΈ
     - μ‹¤μ  λ©μ  ν•¨μλ¥Ό κ·Όμ‚¬ν•λ” λ¨λΈλ΅, μ£Όλ΅ **κ°€μ°μ‹μ• ν”„λ΅μ„Έμ¤(Gaussian Process)**κ°€ μ‚¬μ©λ¨
     - μ΄ λ¨λΈμ€ μ£Όμ–΄μ§„ λ°μ΄ν„°λ΅λ¶€ν„° λ©μ  ν•¨μμ λ¶„ν¬λ¥Ό μ¶”μ •ν•μ—¬, ν•¨μμ λ¶ν™•μ‹¤μ„±μ„ ν‘ν„ν•¨
- 2. νλ“ ν•¨μ
    - μ„λ΅κ²μ΄νΈ λ¨λΈμ„ κΈ°λ°μΌλ΅, λ‹¤μ ν‰κ°€ν•  μ§€μ μ„ κ²°μ •ν•λ” ν•¨μ
    - κΈ°λ€ ν–¥μƒ(Expected Improvement), μµλ€ ν™•λ¥  κ°μ„ (Probability of Improvement), μƒν• μ‹ λΆ° ν•κ³„(Upper Confidence Bound) λ“±μ λ°©λ²•μ΄ μ‚¬μ©λ¨
    - νλ“ ν•¨μλ” νƒμƒ‰(exploration)κ³Ό ν™μ©(exploitation)μ κ· ν•μ„ μ΅°μ ν•μ—¬ μµμ ν™”λ¥Ό ν¨μ¨μ μΌλ΅ μ§„ν–‰ν•¨

Bayesian Optimizationμ μ¥μ κ³Ό ν•κ³„
μ¥μ 
- ν•¨μ ν‰κ°€ λΉ„μ©μ΄ λ†’μ€ λ¬Έμ μ—μ„ μ μ€ ν‰κ°€ νμλ΅ μµμ ν•΄λ¥Ό μ°Ύμ„ μ μμ
- ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”, λ΅λΈν‹±μ¤, κ³Όν•™ μ‹¤ν— μ„¤κ³„ λ“± λ‹¤μ–‘ν• λ¶„μ•Όμ— μ μ© κ°€λ¥

ν•κ³„
- κ³ μ°¨μ› κ³µκ°„μ—μ„λ” μ„λ΅κ²μ΄νΈ λ¨λΈμ λ³µμ΅γ„±μ„±μ΄ μ¦κ°€ν•μ—¬ ν¨μ¨μ„±μ΄ μ €ν•λ  μ μμ
- μ΅μμ΄ λ§μ€ ν•¨μλ‚ λΉ„μ •μƒμ„±μ„ κ°€μ§„ ν•¨μμ—μ„ λ¨λΈλ§ μ–΄λ ¤μΈ μ μμ
  
### 4. Optuna
Oputnaλ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό μ„ν• μ°¨μ„Έλ€ ν”„λ μ„μ›ν¬λ΅, μ‚¬μ©μκ°€ λ™μ μΌλ΅ νƒμƒ‰ κ³µκ°„μ„ μ •μν•κ³  ν¨μ¨μ μΈ μµμ ν™” κ³Όμ •μ„ μν–‰ν•  μ μλ„λ΅ μ„¤κ³„λμ—λ‹¤. 2019λ…„ Akiba λ“±(2019)μ— μν•΄ μ†κ°λμ—μΌλ©°, λ‹¤μ–‘ν• λ¨Έμ‹ λ¬λ‹ λ° λ”¥λ¬λ‹ λ¨λΈ μ„±λ¥ ν–¥μƒμ„ μ„ν•΄ λ„λ¦¬ μ‚¬μ©λκ³  μλ‹¤.

Optunaμ μ£Όμ” νΉμ§•
- μ‚¬μ©μκ°€ μ½”λ“ μ‹¤ν–‰ μ¤‘ **νƒμƒ‰ κ³µκ°„μ„ λ™μ μΌλ΅ κµ¬μ„±**ν•  μ μμ–΄, λ³µμ΅ν• λ¨λΈμ΄λ‚ μ΅°κ±΄λ¶€ ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •μ— μ μ—°ν•κ² λ€μ‘ν•  μ μλ‹¤.
- Optunaλ” Tree-Structured Parzen Estimator(TPE)μ™€ κ°™μ€ μƒν”λ§ κΈ°λ²•μ„ ν†µν•΄ ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ„ ν¨μ¨μ μΌλ΅ νƒμƒ‰ν•λ©°, λΉ„ν¨μ¨μ μΈ μ‹λ„λ¥Ό μ΅°κΈ°μ— μ¤‘λ‹¨(pruning)ν•μ—¬ μμ›μ„ μ μ•½ν•λ‹¤.
- TensorFlow, PyTorch, XGBoost, LightGBM λ“± λ‹¤μ–‘ν• λΌμ΄λΈλ¬λ¦¬μ™€μ ν†µν•©μ„ μ§€μ›ν•λ©°, λ‹¨μΌ λ¨Έμ‹ λ¶€ν„° λ¶„μ‚° ν™κ²½κΉμ§€ λ‹¤μ–‘ν• μ„¤μ •μ—μ„ ν™μ©ν•  μ μλ‹¤.


## μ°Έκ³ λ¬Έν— (References)

<details>
  <summary>μ°Έκ³ λ¬Έν— λ³΄κΈ° π“–</summary>

  - Liashchynskyi, P., & Liashchynskyi, P. (2019). **Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS.**  
    *arXiv preprint arXiv:1912.06059.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/1912.06059?utm_source=chatgpt.com)

  - **Scikit-learn: GridSearchCV**  
    [[κ³µμ‹ λ¬Έμ„]](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

  - **Dremio: What is Grid Search?**  
    [[μ°Έκ³  μλ£]](https://www.dremio.com/wiki/grid-search/)

  ---

  - **Bergstra, J., & Bengio, Y. (2012).**  
    *Random Search for Hyper-Parameter Optimization.*  
    *Journal of Machine Learning Research, 13, 281-305.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf?utm_source=chatgpt.com)

  - **HyperParameter Optimization Algorithm (feat. GP / TPE) - Hoonst**  
    [[λΈ”λ΅κ·Έ λ§ν¬]](https://hoonst.github.io/2020/11/15/Algorithms-for-Hyperparameter-Optimization/?utm_source=chatgpt.com)

  - **[λ…Όλ¬Έμ„Έλ―Έλ‚ 002] ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ©ν‘λ¥Ό μ •μν•κ³  λ¨λΈμ— μμ΅΄μ μ΄μ§€ μ•μ€ Grid Search, Random Search λ°©λ²• λ“±μ„ μ„¤λ…ν•©λ‹λ‹¤ (κΉ€λ„ν• μ—°κµ¬μ›)**  
    [[μμƒ λ§ν¬]](https://www.youtube.com/watch?v=U6bohNT-O5M&utm_source=chatgpt.com)

  ---

  - **Wang, X., Jin, Y., Schmitt, S., & Olhofer, M. (2022).**  
    *Recent Advances in Bayesian Optimization.*  
    *arXiv preprint arXiv:2206.03301.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/2206.03301?utm_source=chatgpt.com)

  - **Paulson, J. A., & Tsay, C. (2024).**  
    *Bayesian Optimization as a Flexible and Efficient Design Framework for Sustainable Process Systems.*  
    *arXiv preprint arXiv:2401.16373.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/2401.16373?utm_source=chatgpt.com)

  - **Wikipedia contributors. (2023).**  
    *Bayesian Optimization.*  
    *Wikipedia, The Free Encyclopedia.*  
    [[μ„ν‚¤λ°±κ³Ό λ§ν¬]](https://en.wikipedia.org/wiki/Bayesian_optimization?utm_source=chatgpt.com)

  - **Brochu, E., Cora, V. M., & de Freitas, N. (2010).**  
    *A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning.*  
    *arXiv preprint arXiv:1012.2599.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/1012.2599?utm_source=chatgpt.com)

  ---

  - **Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019).**  
    *Optuna: A Next-generation Hyperparameter Optimization Framework.*  
    *arXiv preprint arXiv:1907.10902.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/1907.10902?utm_source=chatgpt.com)

  - **Shekhar, S., Bansode, A., & Salim, A. (2022).**  
    *A Comparative Study of Hyper-Parameter Optimization Tools.*  
    *arXiv preprint arXiv:2201.06433.*  
    [[λ…Όλ¬Έ λ§ν¬]](https://arxiv.org/abs/2201.06433?utm_source=chatgpt.com)

  - **Optuna κΈ°μ΄ μ‚¬μ©λ²•.**  
    [[λΈ”λ΅κ·Έ λ§ν¬]](https://velog.io/%40kyyle/Optuna-%EA%B8%B0%EC%B4%88?utm_source=chatgpt.com)

</details>
